# -*- coding: utf-8 -*-
"""AldiAkbarAlimi-NLPSubmission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A1yJt_gHFOI8a8_WbHq1mQwvqE20H8cD
"""

print('Nama                 : Aldi Akbar Alimi')
print('Username dicoding    : alldinosaur')
print('Email                : aldiakbar373@gmail.com')
print('Domisili             : Kab. Cirebon')
print('TTL                  : Brebes, 25 Juni 2001')
print('Pendidikan Terakhir  : SMA Negeri 1 Brebes')
print('Pendidikan Sekarang  : STT Terpadu Nurul Fikri')

import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import pandas as pd
df = pd.read_csv('Corona_NLP_train.csv',encoding="latin1")
df = df.drop(columns=['UserName','ScreenName','Location','TweetAt'])
df.head

print(df['Sentiment'].unique())

for i in range(0,len(df)):
    if(df['Sentiment'][i]=='Extremely Negative'):
        df['Sentiment'][i]='Negative'
    elif(df['Sentiment'][i]=='Extremely Positive'):
        df['Sentiment'][i]='Positive'

print(df['Sentiment'].unique())

category = pd.get_dummies(df.Sentiment)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='Sentiment')
df_baru

import re
import string
def casefolding(OriginalTweet):
  OriginalTweet = OriginalTweet.lower()
  OriginalTweet = OriginalTweet.strip(' ')
  OriginalTweet = re.sub("@\S+", " ", OriginalTweet)
  OriginalTweet=re.sub("https*\S+", " ", OriginalTweet)
  OriginalTweet=re.sub("#\S+", " ", OriginalTweet)
  OriginalTweet=re.sub("\'\w+", '', OriginalTweet)
  OriginalTweet=re.sub('[%s]' % re.escape(string.punctuation), ' ', OriginalTweet)
  OriginalTweet=re.sub(r'\w*\d+\w*', '', OriginalTweet)
  OriginalTweet=re.sub('\s{2,}', " ", OriginalTweet)
  return OriginalTweet
df_baru["OriginalTweet"] = df_baru["OriginalTweet"].apply(casefolding)
df_baru.head

OriginalTweet = df_baru['OriginalTweet'].values
label = df_baru[['Negative', 'Neutral', 'Positive']].values

class Callback(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
        if(logs.get('accuracy')>0.9):
            print("\n Predikasi telah mencapai akurasi lebih dari 90%") 
            self.model.stop_training = True 
 
callbacks = Callback()

OriginalTweet_latih, OriginalTweet_test, label_latih, label_test = train_test_split(OriginalTweet, label, test_size=0.2)

tokenizer = Tokenizer(num_words=5000, oov_token='</OOV>')
tokenizer.fit_on_texts(OriginalTweet_latih) 
tokenizer.fit_on_texts(OriginalTweet_test)
 
sekuens_latih = tokenizer.texts_to_sequences(OriginalTweet_latih)
sekuens_test = tokenizer.texts_to_sequences(OriginalTweet_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

model=tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),    
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(8,activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(16,activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(3,activation='softmax')
])
model.summary()

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

history=model.fit(padded_latih,label_latih,epochs=30,
                      validation_data=(padded_test,label_test),callbacks=[callbacks])

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()