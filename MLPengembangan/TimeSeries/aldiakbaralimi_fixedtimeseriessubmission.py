# -*- coding: utf-8 -*-
"""AldiAkbarAlimi_FixedTimeSeriesSubmission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ojoOBk0spndCzjimYend8o2YBCGUS9gT
"""

print('Nama                 : Aldi Akbar Alimi')
print('Username dicoding    : alldinosaur')
print('Email                : aldiakbar373@gmail.com')
print('Domisili             : Kab. Cirebon')
print('TTL                  : Brebes, 25 Juni 2001')
print('Pendidikan Terakhir  : SMA Negeri 1 Brebes')
print('Pendidikan Sekarang  : STT Terpadu Nurul Fikri')

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import sklearn.preprocessing
from sklearn.model_selection import train_test_split

from keras.layers import Dense,Dropout,SimpleRNN,LSTM
from keras.models import Sequential

df_visualize = pd.read_csv('DOM_hourly.csv', index_col='Datetime', parse_dates=['Datetime'])
df = pd.read_csv('DOM_hourly.csv')
df.head()

#checking missing data
df.isna().sum()

df.shape

df_visualize.plot(figsize=(20,5),legend=True)
plt.title('DOM hourly power consumption data - BEFORE NORMALIZATION',
          fontsize=20);
plt.show()



def normalize_data(df):
    scaler = sklearn.preprocessing.MinMaxScaler()
    df['DOM_MW']=scaler.fit_transform(df['DOM_MW'].values.reshape(-1,1))
    df_visualize['DOM_MW']=scaler.fit_transform(df_visualize['DOM_MW'].values.reshape(-1,1))
    return df

df_norm = normalize_data(df)

df_visualize.plot(figsize=(20,5),legend=True)
plt.title('DOM hourly power consumption data - AFTER NORMALIZATION',
          fontsize=20);
plt.show()

class Callback(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
        if(logs.get('mae')<0.1):
            print("\n Nilai MAE telah dibawah 10%") 
            self.model.stop_training = True 
 
callbacks = Callback()

Datetime = df['Datetime'].values
DOM_MW = df['DOM_MW'].values

Date_latih, Date_test, DOM_MW_latih, DOM_MW_test = train_test_split(Datetime, DOM_MW, test_size=0.2)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(DOM_MW_latih, window_size=60, batch_size=1000, shuffle_buffer=1000)
test_set = windowed_dataset(DOM_MW_test, window_size=60, batch_size=1000, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, activation="tanh", return_sequences=True),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(60, activation="tanh"),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,
                    epochs=10, 
                    validation_data=test_set, 
                    verbose=2,
                    callbacks=[callbacks])

mae = history.history['mae']
val_mae = history.history['val_mae']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(mae, label='Training MAE')
plt.plot(val_mae, label='Validation MAE')
plt.legend(loc='lower right')
plt.title('Training and Validation MAE')

plt.subplot(1, 2, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()